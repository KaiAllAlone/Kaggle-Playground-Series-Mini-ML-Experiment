# -*- coding: utf-8 -*-
"""Calorie_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrLcKKj1MKPBOLf0S8hjjMuJwnG5xhOw

## Import Necessary Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as mp
import seaborn as sn
import time
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualizations
sn.set_style('whitegrid')
mp.rcParams['figure.figsize'] = (12, 6)

from google.colab import drive
drive.mount('/content/drive')

train=pd.read_csv('/content/drive/MyDrive/train.csv')
test=pd.read_csv('/content/drive/MyDrive/test.csv')

"""## Check for Duplicates and NaN values

Duplicates
"""

print(train.duplicated().sum())
print(test.duplicated().sum())

"""NaN Values"""

print(train.isna().sum())
print(test.isna().sum())

"""##Plot the Data Distribution and also Correlation Matrix"""

# Target variable distribution
fig, axes = mp.subplots(1, 2, figsize=(15, 5))
axes[0].hist(train['Calories'], bins=50, edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Calories')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Distribution of Calories')
axes[0].axvline(train['Calories'].mean(), color='red', linestyle='--', label='Mean')
axes[0].axvline(train['Calories'].median(), color='green', linestyle='--', label='Median')
axes[0].legend()

axes[1].boxplot(train['Calories'])
axes[1].set_ylabel('Calories')
axes[1].set_title('Boxplot of Calories')
mp.tight_layout()
mp.show()

numerical_cols = train.select_dtypes(include=[np.number]).columns
if len(numerical_cols) > 1:
    mp.figure(figsize=(10, 10))
    correlation_matrix = train[numerical_cols].corr()
    sn.heatmap(correlation_matrix, annot=True, cmap='viridis', center=0,
                fmt='.2f', square=True, linewidths=1)
    mp.title('Correlation Heatmap')
    mp.tight_layout()
    mp.show()

num_features = [col for col in numerical_cols if col not in ['id', 'Calories']]
if len(num_features) > 0:
    n_cols = 3
    n_rows = (len(num_features) + n_cols - 1) // n_cols
    fig, axes = mp.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_rows > 1 else [axes]

    for idx, col in enumerate(num_features):
        axes[idx].hist(train[col], bins=30, edgecolor='black', alpha=0.7)
        axes[idx].set_xlabel(col)
        axes[idx].set_ylabel('Frequency')
        axes[idx].set_title(f'Distribution of {col}')

    for idx in range(len(num_features), len(axes)):
        fig.delaxes(axes[idx])

    mp.tight_layout()

# Categorical features distribution
categorical_cols = train.select_dtypes(include=['object']).columns
categorical_cols = [col for col in categorical_cols if col != 'id']

if len(categorical_cols) > 0:
    n_cols = 2
    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols
    fig, axes = mp.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    axes = axes.flatten()

    for idx, col in enumerate(categorical_cols):
        if idx < len(axes):
            value_counts = train[col].value_counts()
            sn.countplot(data=train, x=col, ax=axes[idx], palette='Set2', order=value_counts.index)
            axes[idx].set_xlabel(col, fontsize=11)
            axes[idx].set_ylabel('Count', fontsize=11)
            axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')
            axes[idx].tick_params(axis='x', rotation=45)

            # Add count labels on bars
            for container in axes[idx].containers:
                axes[idx].bar_label(container)

    # Remove any unused subplots
    for idx in range(len(categorical_cols), len(axes)):
        fig.delaxes(axes[idx])

    mp.tight_layout()
    mp.show()

"""## Outlier Removal"""

print("\nDetecting and removing outliers using IQR method...")

def remove_outliers(df, columns, target_col='Calories'):
    """Remove outliers using Inter Quartile Range method"""
    df_clean = df.copy()
    initial_rows = len(df_clean)

    for col in columns:
        if col in df_clean.columns and col!=target_col:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1-1.5*IQR
            upper_bound = Q3+1.5*IQR
            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]

    # Also remove outliers from target variable
    if target_col in df_clean.columns:
        Q1 = df_clean[target_col].quantile(0.25)
        Q3 = df_clean[target_col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1-1.5*IQR
        upper_bound = Q3+1.5*IQR
        df_clean = df_clean[(df_clean[target_col] >= lower_bound) & (df_clean[target_col] <= upper_bound)]

    removed_rows = initial_rows - len(df_clean)
    print(f"Removed {removed_rows} outlier rows ({removed_rows/initial_rows*100:.2f}%)")

    return df_clean

# Apply outlier removal to numerical features
outlier_cols = [col for col in num_features if col in train.columns]
train = remove_outliers(train, outlier_cols)

print(f"\nFinal training data shape: {train.shape}")

"""## Feature Engineering , Scaling and Encoding"""

X = train.drop(['Calories'], axis=1)
y = pd.DataFrame(np.log1p(train['Calories']))

def add_feature_cross_terms(df, numerical_features):
    df_new = df.copy()
    df_new['BMI'] = df_new['Weight'] / ((df_new['Height'] / 100) ** 2)

    for i in range(len(numerical_features)):
        for j in range(i + 1, len(numerical_features)):
            feature1 = numerical_features[i]
            feature2 = numerical_features[j]
            cross_term_name = f"{feature1}_x_{feature2}"
            df_new[cross_term_name] = df_new[feature1] * df_new[feature2]

    return df_new

X = add_feature_cross_terms(X, num_features)
test = add_feature_cross_terms(test, num_features)

# Identify feature types
id_cols = ['id'] if 'id' in X.columns else []
numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()
numerical_features = [col for col in numerical_features if col not in id_cols]
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

print(f"\nNumerical features: {numerical_features}")
print(f"Categorical features: {categorical_features}")


# For numerical features: StandardScaler
# For categorical features: OneHotEncoder

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'),
         categorical_features)
    ],
    remainder='drop'  # Drop id column
)

test.columns

print("\n" + "="*80)
print("MODEL TRAINING WITH K-FOLD CROSS-VALIDATION")
print("="*80)
# Configuration
FOLDS = 15
FEATURES = X.columns.tolist()

# Prepare test data
X_test = test.drop(['id'], axis=1, errors='ignore')

# Create preprocessing pipeline
preprocessing_pipeline = Pipeline([
    ('preprocessor', preprocessor)
])

# Apply preprocessing to all data
print("\nApplying preprocessing pipeline...")
preprocessing_pipeline.fit(X)
X_transformed = preprocessing_pipeline.transform(X)
X_test_transformed = preprocessing_pipeline.transform(X_test)

# Convert to DataFrame for easier handling
feature_names_after_preprocessing = (
    numerical_features +
    list(preprocessing_pipeline.named_steps['preprocessor']
         .named_transformers_['cat']
         .get_feature_names_out(categorical_features))
)

X_processed = pd.DataFrame(X_transformed, columns=feature_names_after_preprocessing, index=X.index)
X_test_processed = pd.DataFrame(X_test_transformed, columns=feature_names_after_preprocessing, index=X_test.index)

print(f"Processed features shape: {X_processed.shape}")
print(f"Processed test shape: {X_test_processed.shape}")
print(f"\nPreprocessing Pipeline Steps:")
print(f"  1. StandardScaler for numerical features: {numerical_features}")
print(f"  2. OneHotEncoder for categorical features: {categorical_features}")

# KFold setup
kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)

# Arrays to store predictions
oof = np.zeros(len(X_processed))
pred = np.zeros(len(X_test_processed))
fold_scores = []

# Start CV loop
print(f"\n{'='*80}")
print(f"Starting {FOLDS}-Fold Cross-Validation")
print(f"{'='*80}")

for i, (train_idx, valid_idx) in enumerate(kf.split(X_processed, y)):
    print(f"\n{'#'*10} Fold {i+1}/{FOLDS} {'#'*10}")

    x_train = X_processed.iloc[train_idx].copy()
    y_train = y.iloc[train_idx]
    x_valid = X_processed.iloc[valid_idx].copy()
    y_valid = y.iloc[valid_idx]
    x_test = X_test_processed.copy()

    start = time.time()

    # Train model
    model = XGBRegressor(
        device="cuda:1",  # Use "cpu" if CUDA not available
        max_depth=10,
        colsample_bytree=0.75,
        subsample=0.9,
        n_estimators=2000,
        learning_rate=0.02,
        gamma=0.01,
        max_delta_step=2,
        early_stopping_rounds=100,
        eval_metric="rmse",
        enable_categorical=True,
        random_state=42
    )

    try:
        model.fit(
            x_train, y_train,
            eval_set=[(x_valid, y_valid)],
            verbose=100
        )
    except:
        # Fallback to CPU if CUDA fails
        print("CUDA not available, falling back to CPU...")
        model = XGBRegressor(
            device="cpu",
            max_depth=10,
            colsample_bytree=0.75,
            subsample=0.9,
            n_estimators=2000,
            learning_rate=0.02,
            gamma=0.01,
            max_delta_step=2,
            early_stopping_rounds=100,
            eval_metric="rmse",
            enable_categorical=True,
            random_state=42
        )
        model.fit(
            x_train, y_train,
            eval_set=[(x_valid, y_valid)],
            verbose=100
        )

    # Predict OOF and test
    oof[valid_idx] = model.predict(x_valid)
    pred += model.predict(x_test)

    rmse = np.sqrt(mean_squared_error(y_valid, oof[valid_idx]))
    fold_scores.append(rmse)
    print(f"\nFold {i+1} RMSE: {rmse:.4f}")
    print(f"Training time: {time.time() - start:.1f} sec")

# Average test predictions
pred /= FOLDS
# Final RMSE
full_rmse = np.sqrt(mean_squared_error(y, oof))
print(f"\n{'='*80}")
print(f"CROSS-VALIDATION RESULTS")
print(f"{'='*80}")
print(f"Final OOF RMSE: {full_rmse:.4f}")
print(f"Mean Fold RMSE: {np.mean(fold_scores):.4f}")
print(f"Std Fold RMSE: {np.std(fold_scores):.4f}")
print(f"Min Fold RMSE: {np.min(fold_scores):.4f}")
print(f"Max Fold RMSE: {np.max(fold_scores):.4f}")

# Visualize CV scores
mp.figure(figsize=(14, 6))
mp.subplot(1, 2, 1)
sn.lineplot(x=range(1, FOLDS+1), y=fold_scores, marker='o', linewidth=2, markersize=6, color='steelblue')
mp.axhline(y=np.mean(fold_scores), color='red', linestyle='--', linewidth=2,
            label=f'Mean: {np.mean(fold_scores):.4f}')
mp.fill_between(range(1, FOLDS+1),
                 np.mean(fold_scores) - np.std(fold_scores),
                 np.mean(fold_scores) + np.std(fold_scores),
                 alpha=0.2, color='red', label=f'Â±1 Std: {np.std(fold_scores):.4f}')
mp.xlabel('Fold', fontsize=12)
mp.ylabel('RMSE', fontsize=12)
mp.title(f'{FOLDS}-Fold Cross-Validation Scores', fontsize=14, fontweight='bold')
mp.legend(fontsize=10)
mp.grid(True, alpha=0.3)

# Plot 2: Distribution of fold scores
mp.subplot(1, 2, 2)
sn.histplot(fold_scores, bins=20, kde=True, color='coral')
mp.axvline(np.mean(fold_scores), color='red', linestyle='--', linewidth=2, label='Mean')
mp.axvline(full_rmse, color='green', linestyle='--', linewidth=2, label='OOF RMSE')
mp.xlabel('RMSE', fontsize=12)
mp.ylabel('Frequency', fontsize=12)
mp.title('Distribution of Fold RMSE Scores', fontsize=14, fontweight='bold')
mp.legend(fontsize=10)

mp.tight_layout()
mp.show()

# ============================================================================
# 7. GENERATE PREDICTIONS AND SUBMISSION
# ============================================================================
print("\n" + "="*80)
print("GENERATING PREDICTIONS")
print("="*80)
pred = np.expm1(pred)
print(f"Predictions shape: {pred.shape}")
print(f"Sample predictions: {pred[:5]}")

# Create submission file
submission = pd.DataFrame({
    'id': test['id'],
    'Calories': pred
})

submission.to_csv('submission.csv', index=False)
print("\nSubmission file created: submission.csv")
print(f"Submission shape: {submission.shape}")
print("\nFirst few predictions:")
print(submission.head(10))

# ============================================================================
# 8. FEATURE IMPORTANCE (from last fold model)
# ============================================================================
print("\n" + "="*80)
print("FEATURE IMPORTANCE (Last Fold Model)")
print("="*80)

# Get feature importance from the last trained model
feature_importance = model.feature_importances_

# Create dataframe
importance_df = pd.DataFrame({
    'feature': feature_names_after_preprocessing,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("\nTop 20 Most Important Features:")
print(importance_df.head(20))

# Plot feature importance
mp.figure(figsize=(12, 8))
top_n = min(20, len(importance_df))
colors = sn.color_palette("viridis", top_n)
sn.barplot(data=importance_df.head(top_n), y='feature', x='importance',
            palette=colors, orient='h')
mp.xlabel('Importance', fontsize=12)
mp.ylabel('Feature', fontsize=12)
mp.title(f'Top {top_n} Feature Importances', fontsize=14, fontweight='bold')
mp.tight_layout()
mp.show()

y.values.flatten()

oof

# ============================================================================
# 9. OOF PREDICTIONS ANALYSIS
# ============================================================================
print("\n" + "="*80)
print("OUT-OF-FOLD PREDICTIONS ANALYSIS")
print("="*80)

# Create OOF DataFrame
oof_df = pd.DataFrame({
    'actual': y.values.flatten(),
    'predicted': oof
})
oof_df['residual'] = oof_df['actual'] - oof_df['predicted']

print("\nOOF Predictions Statistics:")
print(oof_df.describe())

# Visualize OOF predictions
fig, axes = mp.subplots(1, 3, figsize=(18, 5))

# Actual vs Predicted
sn.scatterplot(data=oof_df, x='actual', y='predicted', alpha=0.5, ax=axes[0])
axes[0].plot([oof_df['actual'].min(), oof_df['actual'].max()],
             [oof_df['actual'].min(), oof_df['actual'].max()],
             'r--', linewidth=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual Calories', fontsize=12)
axes[0].set_ylabel('Predicted Calories', fontsize=12)
axes[0].set_title('Actual vs Predicted (OOF)', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Residuals distribution
sn.histplot(oof_df['residual'], bins=50, kde=True, ax=axes[1], color='coral')
axes[1].axvline(0, color='red', linestyle='--', linewidth=2)
axes[1].set_xlabel('Residual (Actual - Predicted)', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')

# Residuals vs Predicted
sn.scatterplot(data=oof_df, x='predicted', y='residual', alpha=0.5, ax=axes[2])
axes[2].axhline(0, color='red', linestyle='--', linewidth=2)
axes[2].set_xlabel('Predicted Calories', fontsize=12)
axes[2].set_ylabel('Residual', fontsize=12)
axes[2].set_title('Residual Plot', fontsize=14, fontweight='bold')
axes[2].grid(True, alpha=0.3)

mp.tight_layout()
mp.show()

print("\n" + "="*80)
print("PIPELINE COMPLETE!")
print("="*80)

